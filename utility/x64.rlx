
define void FastMoveMemory(void* To, void* From, i64 Size) asm {
	mov, rcx, rdx
	
	cmp, rdi, rsi
	jlt, MoveDown
	
	dec, rdx
	add, rdi, rdx
	add, rsi, rdx
	
	std
	rep movsb
	
	ret
	
MoveDown:
	cld
	rep movsb
	
	ret
}

define void FastSetMemory8(void* Memory, i64 Value, i64 Count) asm {
	mov, rax, rsi
	mov, rcx, rdx
	
	cld
	rep stosb
	
	ret
}
define void FastSetMemory16(void* Memory, i64 Value, i64 Count) asm {
	mov, rax, rsi
	mov, rcx, rdx
	
	cld
	rep stosw
	
	ret
}
define void FastSetMemory32(void* Memory, i64 Value, i64 Count) asm {
	mov, rax, rsi
	mov, rcx, rdx
	
	cld
	rep stosd
	
	ret
}
define void FastSetMemory64(void* Memory, i64 Value, i64 Count) asm {
	mov, rax, rsi
	mov, rcx, rdx
	
	cld
	rep stosq
	
	ret
}

define i8 InB(i16 Port) asm {
	mov, dx, dix
	inb
	ret
}
define void OutB(i16 Port, i8 Data) asm {
	mov, dx, dix
	mov, a, si
	outb
	ret
}
define i16 InW(i16 Port) asm {
	mov, dx, dix
	inw
	ret
}
define void OutW(i16 Port, i16 Data) asm {
	mov, dx, dix
	mov, ax, six
	outw
	ret
}

define void* GetRSP() asm {
	lea, rax, {rsp - 8} As i64*
	ret
}
define i64* GetRBP() asm {
	mov, rax, rbp
	ret
}

define void WriteMSR(i64 Number, i64 Value) {
	i64 Low := Value & 0xFFFF_FFFF
	i64 High := (Value >> 32) & 0xFFFF_FFFF
	
	asm (rax := Low, rdx := High, rcx := Number) {
		wrmsr
	}
}

define i64 ReadMSR(i64 Number) {
	i64 Low := 0
	i64 High := 0
	
	asm (rdi := &Low, rsi := &High, rcx := Number) {
		rdmsr
		mov, {rdi} As i32*, eax
		mov, {rsi} As i32*, edx
	}
	
	return (High << 32) | Low
}

i64 AMD64_MSR_EFER   := 0xC0000080
i64 AMD64_MSR_STAR   := 0xC0000081
i64 AMD64_MSR_LSTAR  := 0xC0000082
i64 AMD64_MSR_CSTAR  := 0xC0000083
i64 AMD64_MSR_SFMASK := 0xC0000084

struct AMD64Context {
	i64 RIP
	
	i64 RBX
	i64 RCX
	i64 RDX
	i64 RSP
	i64 RBP
	i64 RSI
	i64 RDI
	
	i64 R8
	i64 R9
	i64 R10
	i64 R11
	i64 R12
	i64 R13
	i64 R14
	i64 R15
	
	i64 Flags
}

define i8 SaveContext(AMD64Context* Save) asm {
	pop, rax
	mov, {rdi} As i64*, rax
	
	mov, {rdi + 0x8 } As i64*, rbx
	mov, {rdi + 0x10} As i64*, rcx
	mov, {rdi + 0x18} As i64*, rdx
	mov, {rdi + 0x20} As i64*, rsp
	mov, {rdi + 0x28} As i64*, rbp
	mov, {rdi + 0x30} As i64*, rsi
	mov, {rdi + 0x38} As i64*, rdi
	
	mov, {rdi + 0x40} As i64*, r8
	mov, {rdi + 0x48} As i64*, r9
	mov, {rdi + 0x50} As i64*, r10
	mov, {rdi + 0x58} As i64*, r11
	mov, {rdi + 0x60} As i64*, r12
	mov, {rdi + 0x68} As i64*, r13
	mov, {rdi + 0x70} As i64*, r14
	mov, {rdi + 0x78} As i64*, r15
	
	emit, 0x48
	emit, 0x9C ; PUSHFQ
	pop, rbx
	mov, {rdi + 0x80} As i64*, rbx
	
	mov, rbx, 0
	xchg, rax, rbx
	
	jmp, rbx
}

define void RestoreContext(AMD64Context* Save) asm {
	mov, rsp, {rdi + 0x20} As i64* ; Restore RSP/RBP
	mov, rbp, {rdi + 0x28} As i64* 
	
	push, {rdi} As i64* ; Push RIP onto the restored stack, so we can `ret` to it
	
	mov, rbx, {rdi + 0x8 } As i64*
	mov, rcx, {rdi + 0x10} As i64*
	mov, rdx, {rdi + 0x18} As i64*
	mov, rsi, {rdi + 0x30} As i64*
	mov, rdi, {rdi + 0x38} As i64*

	mov, r8 , {rdi + 0x40} As i64*
	mov, r9 , {rdi + 0x48} As i64*
	mov, r10, {rdi + 0x50} As i64*
	mov, r11, {rdi + 0x58} As i64*
	mov, r12, {rdi + 0x60} As i64*
	mov, r13, {rdi + 0x68} As i64*
	mov, r14, {rdi + 0x70} As i64*
	mov, r15, {rdi + 0x78} As i64*
	
	push, {rdi + 0x80} As i64*
	emit, 0x48
	emit, 0x9D ; POPFQ
	
	mov, rax, 1
	
	ret
}
